from collections import defaultdict
import os
import subprocess
import uuid

import numpy as np
import torch

from big_rl.generic.script import main as train_generic
from big_rl.generic.script import init_arg_parser
from big_rl.generic.evaluate_model import main as eval_generic
from big_rl.generic.evaluate_model import init_arg_parser as init_eval_arg_parser


DEBUG = True
MODEL_CONFIG_DIR = './big_rl_experiments/exp1/configs/models'
ENV_CONFIG_DIR = './big_rl_experiments/exp1/configs/envs'
TOTAL_NUM_TASKS = 9
RUN_ID_PREFIX = 'exp1-2023_12_09-'
RESULTS_DIR = os.path.join(os.environ['HOME'], 'results', 'big_rl_experiments', 'exp1', 'debug')
NUM_RANDOM_EVALS = 5 # Number of times to evaluate randomly initialized models for baseline performance

#CHECKPOINT_DIR = './checkpoints'
CHECKPOINT_DIR = os.path.join(RESULTS_DIR, 'checkpoints')
EVAL_RESULTS_DIR = os.path.join(RESULTS_DIR, 'eval_results')
EVAL_TRAINING_TASK_RESULTS_DIR = os.path.join(RESULTS_DIR, 'eval_training_task_results')
EVAL_RANDOM_RESULTS_DIR = os.path.join(RESULTS_DIR, 'eval_random_results')
PLOTS_DIR = os.path.join(RESULTS_DIR, 'plots')


def generate_training_args(num_tasks: int, required_tasks: list[int] | None = None) -> list[str]:
    # Get all task config files
    filenames = os.listdir(os.path.join(ENV_CONFIG_DIR, 'train/autogenerated'))
    filenames = [f for f in filenames if f.endswith('.yaml')]
    np.random.shuffle(filenames)

    # Find the first file that has the required tasks
    env_config = None
    model_checkpoint = None
    run_id = None
    for filename in filenames:
        # Count number of tasks (1s) in the filename
        num_tasks_in_filename = sum([int(c) for c in filename.split('.')[0]])
        if num_tasks_in_filename != num_tasks:
            continue
        # Check if the required tasks are in the filename
        if required_tasks is not None:
            if not all(bool(int(filename[t])) for t in required_tasks):
                continue
        # Found a file that matches the criteria
        model_checkpoint = os.path.join(CHECKPOINT_DIR, filename.replace('.yaml', '.pt'))
        # Check if the model checkpoint exists
        # If it does, it means we've already trained on this set of tasks, so we should look for another one
        if os.path.exists(model_checkpoint):
            model_checkpoint = None
            continue
        # If it doesn't exist, then we're good to go
        env_config = os.path.join(ENV_CONFIG_DIR, 'train/autogenerated', filename)
        run_id = RUN_ID_PREFIX + filename.replace('.yaml', '')
        break
    if env_config is None:
        raise ValueError('No matching config found')

    args = [
        '--env-config', env_config,
        '--model-config',
            os.path.join(MODEL_CONFIG_DIR, ('debug.yaml' if DEBUG else 'model.yaml')),
        '--model-checkpoint', model_checkpoint,
        '--run-id', run_id,
        '--max-steps-total',
            ('30000' if DEBUG else '50_000_000'),
        '--cuda',
    ]
    if not DEBUG:
        args.append('--wandb')
    return args


def run_local(args: list[str]):
    parser = init_arg_parser()
    args_ns = parser.parse_args(args)
    #print(' ', args_ns.env_config)
    train_generic(args_ns)


def run_subprocess(args: list[str]):
    script = 'big_rl/generic/script.py'
    cmd = f'python3 {script} {" ".join(args)}'

    print(cmd)

    p = subprocess.Popen(cmd)
    p.wait()


def run_slurm(args: list[str]):
    raise NotImplementedError()


def generate_eval_args(model_checkpoint: str) -> list[str] | None:
    # Test set is the complement of the training set
    env_config = os.path.join(
        ENV_CONFIG_DIR,
        'test/autogenerated',
        ''.join([
            '1' if c == '0' else '0' if c == '1' else c
            for c in os.path.basename(model_checkpoint)
        ]).replace('.pt', '.yaml')
    )
    print(f'{os.path.basename(model_checkpoint)} -> {env_config}')

    results = os.path.join(EVAL_RESULTS_DIR, os.path.basename(model_checkpoint))
    if os.path.exists(results):
        print(f'  {results} already exists')
        return None

    args = [
        '--env-config', env_config,
        '--model-config',
            os.path.join(MODEL_CONFIG_DIR, ('debug.yaml' if DEBUG else 'model.yaml')),
        '--model', model_checkpoint,
        '--results', os.path.join(EVAL_RESULTS_DIR, os.path.basename(model_checkpoint)),
        '--num-episodes', '10',
        '--no-video',
    ]
    return args


def generate_eval_training_task_args(model_checkpoint: str) -> list[str] | None:
    # Test models on the training tasks.
    # Get a score for normalization purposes.
    env_config = os.path.join(ENV_CONFIG_DIR, 'test/autogenerated', os.path.basename(model_checkpoint).replace('.pt', '.yaml'))

    results = os.path.join(EVAL_TRAINING_TASK_RESULTS_DIR, os.path.basename(model_checkpoint))
    if os.path.exists(results):
        print(f'  {results} already exists')
        return None

    args = [
        '--env-config', env_config,
        '--model-config',
            os.path.join(MODEL_CONFIG_DIR, ('debug.yaml' if DEBUG else 'model.yaml')),
        '--model', model_checkpoint,
        '--results', os.path.join(EVAL_TRAINING_TASK_RESULTS_DIR, os.path.basename(model_checkpoint)),
        '--num-episodes', '10',
        '--no-video',
    ]
    return args


def generate_random_args() -> list[str]:
    # Test randomly initialized models.
    # Get a score for normalization purposes.
    env_config_dir = os.path.join(ENV_CONFIG_DIR, 'test/autogenerated')
    # Choose first file and replace 0s with 1s to get the set with all tasks
    env_config = os.path.join(env_config_dir, os.listdir(env_config_dir)[0].replace('0', '1'))
    # Random results file name
    results = os.path.join(EVAL_RANDOM_RESULTS_DIR, f'{uuid.uuid4()}.pt')

    args = [
        '--env-config', env_config,
        '--model-config',
            os.path.join(MODEL_CONFIG_DIR, ('debug.yaml' if DEBUG else 'model.yaml')),
        '--results', results,
        '--num-episodes', '1',
        '--no-video',
    ]
    return args


def eval_local(args: list[str]):
    parser = init_eval_arg_parser()
    args_ns = parser.parse_args(args)
    print(' ', args_ns.env_config)
    eval_generic(args_ns)


def get_random_performance():
    data = defaultdict(list)
    for filename in os.listdir(EVAL_RANDOM_RESULTS_DIR):
        if not filename.endswith('.pt'):
            continue
        results = torch.load(os.path.join(EVAL_RANDOM_RESULTS_DIR, filename))
        for task_name, result in results.items():
            data[task_name].extend(r['episode_reward'].item() for r in result)
    return {k: np.mean(v) for k, v in data.items()}


def get_single_task_performance():
    data = defaultdict(list)
    for filename in os.listdir(EVAL_TRAINING_TASK_RESULTS_DIR):
        if not filename.endswith('.pt'):
            continue
        results = torch.load(os.path.join(EVAL_TRAINING_TASK_RESULTS_DIR, filename))
        for task_name, result in results.items():
            data[task_name].extend(r['episode_reward'].item() for r in result)
    return {k: np.mean(v) for k, v in data.items()}


def plot_results():
    data_raw = defaultdict(lambda: defaultdict(list))
    for filename in os.listdir(EVAL_RESULTS_DIR):
        if not filename.endswith('.pt'):
            continue
        num_tasks = os.path.basename(filename).split('.')[0].count('1')
        results = torch.load(os.path.join(EVAL_RESULTS_DIR, filename))
        for task_name, result in results.items():
            data_raw[num_tasks][task_name].extend(r['episode_reward'].item() for r in result)
        if len(data_raw) >= 8:
            break

    random_performance = get_random_performance()
    single_task_performance = get_single_task_performance()

    data_normalized = defaultdict(list)
    for num_tasks, data in data_raw.items():
        for task_name, d in data.items():
            min_r = random_performance[task_name]
            max_r = single_task_performance[task_name]
            data_normalized[num_tasks].extend((x - min_r) / (max_r - min_r) for x in d)

    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt

    labels = sorted(data_normalized.keys())
    data = [data_normalized[k] for k in labels]

    plt.figure()
    plt.title('Generalization performance of models trained on different numbers of tasks')
    plt.xlabel('Number of training tasks')
    plt.ylabel('Normalized performance')
    plt.grid(axis='y', which='both', linestyle='--')
    plt.boxplot(data, labels=labels)
    filename = os.path.join(PLOTS_DIR, 'boxplot.png')
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    plt.savefig(filename)

    print(f'Saved plot to {os.path.abspath(filename)}')


def main():
    for num_tasks in range(1, TOTAL_NUM_TASKS):
        print(f'Generating args for {num_tasks} tasks')
        for required_tasks in range(TOTAL_NUM_TASKS):
            # Make sure that each task appears at least once in the training set
            args = generate_training_args(num_tasks, [required_tasks])

            run_local(args)
            #run_subprocess(args)
            #run_slurm(args)

    # Evaluate randomly initialized models
    for _ in range(NUM_RANDOM_EVALS):
        args = generate_random_args()
        eval_local(args)

    # Evaluate all models that were trained on a single task
    # Test on their training task
    for filename in os.listdir(CHECKPOINT_DIR):
        if filename.count('1') != 1:
            continue
        args = generate_eval_training_task_args(os.path.join(CHECKPOINT_DIR, filename))
        if args is None:
            continue
        eval_local(args)

    # Loop through all checkpoint files and evaluate them
    for filename in os.listdir(CHECKPOINT_DIR):
        if filename.count('0') == 0: # Skip models that were trained on all tasks because the test set would be empty
            continue
        args = generate_eval_args(os.path.join(CHECKPOINT_DIR, filename))
        if args is None:
            continue
        eval_local(args)

    # Plot the results
    plot_results()


if __name__ == '__main__':
    main()
