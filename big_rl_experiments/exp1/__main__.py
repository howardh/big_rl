import argparse
from collections import defaultdict
import datetime
import functools
import itertools
import os
import subprocess
import sys
from typing import Generator, NamedTuple
import uuid

import numpy as np
import torch
from simple_slurm import Slurm
from tabulate import tabulate

from big_rl.generic.script import main as train_generic, Callbacks
from big_rl.generic.script import init_arg_parser as init_train_arg_parser
from big_rl.generic.evaluate_model import main as eval_generic
from big_rl.generic.evaluate_model import init_arg_parser as init_eval_arg_parser


SHELL = '/bin/bash' # bash is needed because `trap` behaves differently in sh and the `module` command isn't available in sh.

#GRES = 'gpu:rtx8000:1'
GRES = 'gpu:a100l.2g.20gb:1'
#GRES = 'gpu:a100l.3g.40gb:1'
#GRES = 'gpu:v100:1'


#DEBUG = False
#MODEL_CONFIG_DIR = './big_rl_experiments/exp1/configs/models'
#ENV_CONFIG_DIR = './big_rl_experiments/exp1/configs/envs'
TOTAL_NUM_TASKS = 9
#RUN_ID_PREFIX = 'exp1-2023_12_11-'
#RUN_ID_PREFIX = 'debug-'
#NUM_RANDOM_EVALS = 5 if DEBUG else 100 # Number of times to evaluate randomly initialized models for baseline performance
#MAX_STEPS = 50_000_000
#MAX_STEPS = 1_000_000

#RESULTS_DIR = os.path.join(os.environ['HOME'], 'results', 'big_rl_experiments', 'exp1', RUN_ID_PREFIX[:-1])
#CHECKPOINT_DIR = os.path.join(RESULTS_DIR, 'checkpoints')
#EVAL_RESULTS_DIR = os.path.join(RESULTS_DIR, 'eval_results')
#EVAL_TRAINING_TASK_RESULTS_DIR = os.path.join(RESULTS_DIR, 'eval_training_task_results')
#EVAL_RANDOM_RESULTS_DIR = os.path.join(RESULTS_DIR, 'eval_random_results')
#PLOTS_DIR = os.path.join(RESULTS_DIR, 'plots')

EVAL_RESULTS_SUBDIR = 'eval_results'
EVAL_SHUFFLED_OBS_RESULTS_SUBDIR = 'eval_shuffled_obs_results'
EVAL_SHUFFLED_ACTION_RESULTS_SUBDIR = 'eval_shuffled_action_results'
EVAL_SHUFFLED_OBS_AND_ACTION_RESULTS_SUBDIR = 'eval_shuffled_obs_and_action_results'
EVAL_OCCLUDED_OBS_100_RESULTS_SUBDIR = 'eval_occluded_obs_100_results'


class ResultsDir(NamedTuple):
    default: str
    shuffled_obs: str
    shuffled_action: str
    shuffled_obs_and_action: str
    occluded_obs_100: str
    occluded_obs_action_reward_100: str

    @classmethod
    def from_results_dir(cls, results_dir: str):
        return cls(
            default=os.path.join(results_dir, 'eval_results'),
            shuffled_obs=os.path.join(results_dir, 'eval_shuffled_obs_results'),
            shuffled_action=os.path.join(results_dir, 'eval_shuffled_action_results'),
            shuffled_obs_and_action=os.path.join(results_dir, 'eval_shuffled_obs_and_action_results'),
            occluded_obs_100=os.path.join(results_dir, 'eval_occluded_obs_100_results'),
            occluded_obs_action_reward_100=os.path.join(results_dir, 'eval_occluded_obs_action_reward_100_results'),
        )


##################################################
# Training
##################################################

@functools.cache
def get_task_configs(env_config_dir: str) -> dict:
    """ Get all task config files in the provided directory.
    
    This is implemented as a cached function so that we can keep track of which tasks have been used. The 'unavailable' list is only initialized once, so it maintains all changes that are applied to it.
    """
    filenames = os.listdir(os.path.join(env_config_dir, 'train/autogenerated'))
    filenames = [f for f in filenames if f.endswith('.yaml')]
    np.random.shuffle(filenames)
    
    return {
        'all': filenames,
        'unavailable': [],
    }


def generate_training_args(num_tasks: int, exp_id, env_config_dir, model_config, checkpoint_dir, max_steps, required_tasks: list[int] | None = None, debug: bool = False) -> list[str] | None:
    # Get all task config files
    #filenames = os.listdir(os.path.join(env_config_dir, 'train/autogenerated'))
    #filenames = [f for f in filenames if f.endswith('.yaml')]
    #np.random.shuffle(filenames)
    filenames = get_task_configs(env_config_dir)

    # Find the first file that has the required tasks
    env_config = None
    model_checkpoint = None
    run_id = None
    already_trained = False
    for filename in filenames['all']:
        # Count number of tasks (1s) in the filename
        num_tasks_in_filename = sum([int(c) for c in filename.split('.')[0]])
        if num_tasks_in_filename != num_tasks:
            continue
        # Check if the required tasks are in the filename
        if required_tasks is not None:
            if not all(bool(int(filename[t])) for t in required_tasks):
                continue
        # Check if the file is available
        if filename in filenames['unavailable']:
            continue
        # It is available. Reserve it.
        filenames['unavailable'].append(filename)
        # Check if the model checkpoint exists
        # If it does, it means we've already trained on this set of tasks, so we should look for another one
        model_checkpoint = os.path.join(checkpoint_dir, filename.replace('.yaml', '.pt'))
        if os.path.exists(model_checkpoint):
            model_checkpoint = None
            already_trained = True # Mark that we already trained on a set of tasks matching the criteria
            continue
        # If it doesn't exist, then we're good to go
        env_config = os.path.join(env_config_dir, 'train/autogenerated', filename)
        run_id = f'{exp_id}-{filename.replace(".yaml", "")}'
        break
    if env_config is None:
        if already_trained:
            return None
        else:
            raise ValueError('No matching config found')

    args = [
        '--env-config', env_config,
        '--model-config', model_config,
        '--model-checkpoint', model_checkpoint,
        '--checkpoint-interval', '1_000_000',
        '--run-id', run_id,
        '--wandb-id', run_id,
        '--max-steps-total',
            ('30000' if debug else str(max_steps)),
        '--cuda',
    ]
    if not debug:
        args.append('--wandb')
    return args


def get_all_resume_training_args(num_tasks: int, exp_id, env_config_dir, model_config, checkpoint_dir, max_steps, required_tasks: list[int] | None = None, debug: bool = False) -> Generator[list[str], None, None]:
    """ Get all arguments for resuming training on all tasks fitting the given criteria. """
    raise NotImplementedError('Untested.')

    # Get all task config files
    filenames = get_task_configs(env_config_dir)

    # Find all files that include the required tasks
    env_config = None
    model_checkpoint = None
    run_id = None
    for filename in filenames['all']:
        # Count number of tasks (1s) in the filename
        num_tasks_in_filename = sum([int(c) for c in filename.split('.')[0]])
        if num_tasks_in_filename != num_tasks:
            continue
        # Check if the required tasks are in the filename
        if required_tasks is not None:
            if not all(bool(int(filename[t])) for t in required_tasks):
                continue
        # Check if the file is available
        if filename in filenames['unavailable']:
            continue
        # It is available. Reserve it.
        filenames['unavailable'].append(filename)
        # Check if the model checkpoint exists
        # If it doesn't, then we haven't started training on this task set, so we skip
        model_checkpoint = os.path.join(checkpoint_dir, filename.replace('.yaml', '.pt'))
        if not os.path.exists(model_checkpoint):
            continue
        # If it does exist, then we're good to go
        env_config = os.path.join(env_config_dir, 'train/autogenerated', filename)
        run_id = f'{exp_id}-{filename.replace(".yaml", "")}'

        args = [
            '--env-config', env_config,
            '--model-config', model_config,
            '--model-checkpoint', model_checkpoint,
            '--checkpoint-interval', '1_000_000',
            '--run-id', run_id,
            '--wandb-id', run_id,
            '--max-steps-total',
                ('30000' if debug else str(max_steps)),
            '--cuda',
        ]
        if not debug:
            args.append('--wandb')
        yield args


def run_local(args: list[str]):
    parser = init_train_arg_parser()
    args_ns = parser.parse_args(args)
    #print(' ', args_ns.env_config)
    train_generic(args_ns)


def run_subprocess(args: list[str]):
    """ Train a model in a subprocess. This is preferable to running on the main process because the training involves logging to W&B, which doesn't like getting re-initialized in the same process. """

    script = 'big_rl/generic/script.py'
    cmd = f'python3 {script} {" ".join(args)}'

    print(cmd)

    p = subprocess.Popen(cmd, shell=True) # if shell=True, then the subprocesses will have the same environment variables as this process. Needed to pass the CUDA_VISIBLE_DEVICES variable.
    p.wait()


def run_slurm(args: list[str]):
    slurm = Slurm(
        job_name='train',
        cpus_per_task=8,
        mem='8G',
        gres=[GRES],
        output='/network/scratch/h/huanghow/slurm/%A_%a.out',

        # Run for five days
        array='1-5%1',
        time=datetime.timedelta(days=1, hours=0, minutes=0, seconds=0),

        #partition='main',
        signal='USR1@120', # Send a signal to the job 120 seconds before it is killed
    )
    #slurm.add_cmd('module load libffi') # Fixes the "ImportError: libffi.so.6: cannot open shared object file: No such file or directory" error
    slurm.add_cmd('module load python/3.10')
    slurm.add_cmd('source big_rl/ENV/bin/activate')
    slurm.add_cmd('export PYTHONUNBUFFERED=1')
    # https://stackoverflow.com/questions/76348342/how-to-use-trap-in-my-sbatch-bash-job-script-in-compute-canada
    slurm.add_cmd("trap 'echo SIGUSR1 1>&2' SIGUSR1") # Handles time limit
    slurm.add_cmd("trap 'echo SIGUSR1 1>&2' SIGTERM") # Handles preemption (I think this is needed if PreemptParameters isn't set with send_user_signal enabled. Check if it's set in /etc/slurm/slurm.conf)
    print('-'*80)
    print(slurm.script(shell=SHELL))
    print('-'*80)
    job_id = slurm.sbatch('srun python big_rl/generic/script.py ' + ' '.join(args), shell=SHELL)
    return job_id


##################################################
# Evaluation
##################################################


def generate_eval_args(model_checkpoint: str, output_dir, env_config_dir, model_config, debug) -> list[str] | None:
    # Test set is the complement of the training set
    env_config = os.path.join(
        env_config_dir,
        ''.join([
            '1' if c == '0' else '0' if c == '1' else c
            for c in os.path.basename(model_checkpoint)
        ]).replace('.pt', '.yaml')
    )
    print(f'{os.path.basename(model_checkpoint)} -> {env_config}')

    results = os.path.join(output_dir, os.path.basename(model_checkpoint))
    if os.path.exists(results):
        print(f'  {results} already exists')
        return None

    args = [
        '--env-config', env_config,
        '--model-config', model_config,
        '--model', model_checkpoint,
        '--results', results,
        '--num-episodes', '10',
        '--no-video',
    ]
    return args


def generate_eval_training_task_args(model_checkpoint: str, output_dir: str, env_config_dir, model_config, debug) -> list[str] | None:
    # Test models on the training tasks.
    # Get a score for normalization purposes.
    env_config = os.path.join(env_config_dir, 'test/autogenerated', os.path.basename(model_checkpoint).replace('.pt', '.yaml'))

    results = os.path.join(output_dir, os.path.basename(model_checkpoint))
    if os.path.exists(results):
        print(f'  {results} already exists')
        return None

    args = [
        '--env-config', env_config,
        '--model-config', model_config,
        '--model', model_checkpoint,
        '--results', results,
        '--num-episodes', '10',
        '--no-video',
    ]
    return args


def generate_random_args(output_dir, env_config_dir, model_config, debug) -> list[str]:
    # Test randomly initialized models.
    # Get a score for normalization purposes.
    env_config_dir = os.path.join(env_config_dir, 'test/autogenerated')
    # Choose first file and replace 0s with 1s to get the set with all tasks
    env_config = os.path.join(env_config_dir, os.listdir(env_config_dir)[0].replace('0', '1'))
    # Random results file name
    results = os.path.join(output_dir, f'{uuid.uuid4()}.pt')

    args = [
        '--env-config', env_config,
        '--model-config', model_config,
        '--results', results,
        '--num-episodes', '1',
        '--no-video',
    ]
    return args


def eval_local(args: list[str]):
    parser = init_eval_arg_parser()
    args_ns = parser.parse_args(args)
    print(' ', args_ns.env_config)
    eval_generic(args_ns)


def eval_slurm(args: list[str], after: list[int] = []):
    slurm_kwargs = {}
    if len(after) > 0:
        slurm_kwargs['dependency'] = dict(afterok=':'.join(str(job_id) for job_id in after))
    slurm = Slurm(
        job_name='eval',
        cpus_per_task=8,
        mem='8G',
        output='/network/scratch/h/huanghow/slurm/%A_%a.out',
        time=datetime.timedelta(days=0, hours=2, minutes=0, seconds=0),
        partition='long-cpu',
        **slurm_kwargs,
    )
    #slurm.add_cmd('module load libffi') # Fixes the "ImportError: libffi.so.6: cannot open shared object file: No such file or directory" error
    slurm.add_cmd('module load python/3.10')
    slurm.add_cmd('source big_rl/ENV/bin/activate')
    slurm.add_cmd('export PYTHONUNBUFFERED=1')
    cmd = 'python big_rl/generic/evaluate_model.py ' + ' '.join(args)
    print('-'*80)
    print(slurm.script(shell=SHELL))
    print(cmd)
    print('-'*80)
    job_id = slurm.sbatch(cmd, shell=SHELL)
    return job_id


def eval_random_slurm(argv):
    argv = filter(lambda arg: arg != '--slurm', argv)
    slurm = Slurm(
        job_name='eval_random',
        cpus_per_task=8,
        mem='8G',
        output='/network/scratch/h/huanghow/slurm/%A_%a.out',
        time=datetime.timedelta(days=1, hours=0, minutes=0, seconds=0),
        partition='long-cpu',
    )
    #slurm.add_cmd('module load libffi') # Fixes the "ImportError: libffi.so.6: cannot open shared object file: No such file or directory" error
    slurm.add_cmd('module load python/3.10')
    slurm.add_cmd('source big_rl/ENV/bin/activate')
    slurm.add_cmd('export PYTHONUNBUFFERED=1')
    print('-'*80)
    print(slurm.script(shell=SHELL))
    print('-'*80)
    job_id = slurm.sbatch('python big_rl_experiments/exp1/__main__.py eval_random ' + ' '.join(argv))
    return job_id


##################################################
# Plotting
##################################################


def get_performance_episode(results_dir):
    if not os.path.exists(results_dir):
        return {}

    data = defaultdict(list)
    for filename in os.listdir(results_dir):
        if not filename.endswith('.pt'):
            continue
        results = torch.load(os.path.join(results_dir, filename))
        for task_name, result in results.items():
            data[task_name].extend(r['episode_reward'].item() for r in result)
    return {k: np.mean(v) for k, v in data.items()}


def get_performance_episode_by_num_tasks(results_dir) -> dict[int, dict[str, list[float]]]:
    if not os.path.exists(results_dir):
        return {}

    data: dict[int, dict] = defaultdict(lambda: defaultdict(list)) # {num_tasks: {task_name: [performance]}}
    for filename in os.listdir(results_dir):
        if not filename.endswith('.pt'):
            continue
        num_tasks = os.path.basename(filename).split('.')[0].count('1')
        results = torch.load(os.path.join(results_dir, filename))
        for task_name, result in results.items():
            data[num_tasks][task_name].extend(r['episode_reward'].item() for r in result)
    return data


def get_performance_over_time(results_dir):
    if not os.path.exists(results_dir):
        return {}

    data = defaultdict(list)
    for filename in os.listdir(results_dir):
        if not filename.endswith('.pt'):
            continue
        results = torch.load(os.path.join(results_dir, filename))
        for task_name, result in results.items():
            data[task_name].extend(np.concatenate(r['results']['reward']) for r in result)
    return {k: np.mean(v) for k, v in data.items()}


def performance_table(results_dir, random_performance, single_task_performance):
    eval_performance = get_performance_episode_by_num_tasks(results_dir)

    eval_performance_mean = {}
    for num_tasks, data in eval_performance.items():
        eval_performance_mean[num_tasks] = {}
        for task_name, d in data.items():
            eval_performance_mean[num_tasks][task_name] = np.mean(d)
    num_tasks = sorted(eval_performance.keys())

    RED = '\033[1;31m'
    NOCOLOR = '\033[0m'

    def get_perf(num_tasks, task_name, normalize=False):
        if task_name not in eval_performance_mean[num_tasks]:
            return 'n/a'
        perf = eval_performance_mean[num_tasks][task_name]
        normed_perf = (perf - random_performance[task_name]) / (single_task_performance[task_name] - random_performance[task_name])

        if perf < random_performance[task_name]:
            if normalize:
                return f'{RED}{normed_perf:.2f}{NOCOLOR}'
            else:
                return f'{RED}{perf:.2f}{NOCOLOR}'

        if normalize:
            return normed_perf
        else:
            return perf

    # Render performance in a table
    row_headings = sorted(set(list(random_performance.keys()) + list(single_task_performance.keys())))
    col_headings = ['', 'Random', 'Single Task'] + [f'{x} Tasks' for x in num_tasks] + [f'{x} Tasks (N)' for x in num_tasks]
    table_data = [
        [
            task_name,
            random_performance.get(task_name, 'n/a'),
            single_task_performance.get(task_name, 'n/a'),
        ] + [
            get_perf(x, task_name) for x in num_tasks
        ] + [
            get_perf(x, task_name, normalize=True) for x in num_tasks
        ]
        for task_name in row_headings
    ]
    table = tabulate(table_data, headers=col_headings, tablefmt='simple_grid')
    print(table)


def plot_performance_episode(eval_results_dir, output_dir, random_performance, single_task_performance):
    # Gather performance on the eval tasks
    # Organize by number of training tasks, and the test task
    data_raw = get_performance_episode_by_num_tasks(eval_results_dir)
    #data_raw = defaultdict(lambda: defaultdict(list)) # {num_tasks: {task_name: [performance]}}
    #for filename in os.listdir(eval_results_dir):
    #    if not filename.endswith('.pt'):
    #        continue
    #    num_tasks = os.path.basename(filename).split('.')[0].count('1')
    #    results = torch.load(os.path.join(eval_results_dir, filename))
    #    for task_name, result in results.items():
    #        data_raw[num_tasks][task_name].extend(r['episode_reward'].item() for r in result)
    #    if len(data_raw) >= 8: # XXX: ??? What is this for? I don't remember why it's here.
    #        break

    # Normalize data such that 0 is random performance and 1 is the performance of a model trained on that task alone
    data_normalized = defaultdict(list)
    for num_tasks, data in data_raw.items():
        for task_name, d in data.items():
            min_r = random_performance[task_name]
            max_r = single_task_performance[task_name]
            data_normalized[num_tasks].extend((x - min_r) / (max_r - min_r) for x in d)

    # Render generalization performance in a table
    row_headings = sorted(data_normalized.keys())
    col_headings = ['Num Training Tasks', 'Mean', 'Std', 'n']
    table_data = [[
            num_tasks,
            np.mean(data_normalized[num_tasks]),
            np.std(data_normalized[num_tasks]),
            len(data_normalized[num_tasks]),
        ] for num_tasks in row_headings
    ]
    table = tabulate(table_data, headers=col_headings, tablefmt='simple_grid')
    print('Generalization Performance')
    print(table)

    # Render Welch's t-test results in a table
    import scipy
    def welch(data1, data2):
        """ Check if data1 is smaller than data2 """
        t, p = scipy.stats.ttest_ind(data1, data2, equal_var=False, alternative='less')
        return p
    row_headings = sorted(data_normalized.keys())
    col_headings = [''] + row_headings
    table_data = [
            [n1] + 
            [welch(data_normalized[n1], data_normalized[n2]) if n2 >= n1 else '' for n2 in row_headings]
            for n1 in row_headings
    ]
    table = tabulate(table_data, headers=col_headings, tablefmt='simple_grid')
    print('Welch\'s t-test p-values (one-tailed)')
    print(table)

    # Render generalization performance as box plot
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt

    labels = sorted(data_normalized.keys())
    data = [data_normalized[k] for k in labels]

    plt.figure()
    plt.title('Generalization performance of models trained on different numbers of tasks')
    plt.xlabel('Number of training tasks')
    plt.ylabel('Normalized performance')
    plt.grid(axis='y', which='both', linestyle='--')
    plt.boxplot(data, labels=labels)
    filename = os.path.join(output_dir, 'boxplot.png')
    os.makedirs(os.path.dirname(filename), exist_ok=True)
    plt.savefig(filename)

    print(f'Saved plot to {os.path.abspath(filename)}')


def plot_performance_over_time(eval_results_dir, output_dir, random_performance, single_task_performance):
    # Gather performance on the eval tasks
    data_raw = defaultdict(lambda: defaultdict(list)) # {num_tasks: {task_name: [performance]}}
    for filename in os.listdir(eval_results_dir):
        if not filename.endswith('.pt'):
            continue
        num_tasks = os.path.basename(filename).split('.')[0].count('1')
        results = torch.load(os.path.join(eval_results_dir, filename))
        for task_name, result in results.items():
            data_raw[num_tasks][task_name].extend(np.concatenate(r['results']['reward']) for r in result)

    # Normalize data such that 0 is random performance and 1 is the performance of a model trained on that task alone
    data_normalized = defaultdict(lambda: defaultdict(list))
    for num_tasks, data in data_raw.items():
        for task_name, d in data.items():
            min_r = random_performance[task_name]
            max_r = single_task_performance[task_name]
            data_normalized[num_tasks][task_name].extend((x - min_r) / (max_r - min_r) for x in d)

    # Average across episodes, trimmed to the shortest length episode
    data_avg_short = defaultdict(dict)
    for num_tasks, data in data_raw.items():
        for task_name, d in data.items():
            #max_len = max(len(x) for x in d)
            min_len = min(len(x) for x in d)
            data_avg_short[num_tasks][task_name] = np.mean(np.stack([x[:min_len] for x in d]), axis=0)

    # Average across episodes, padded to the longest length episode
    data_avg_long = defaultdict(dict)
    for num_tasks, data in data_raw.items():
        for task_name, d in data.items():
            max_len = max(len(x) for x in d)
            data_avg_long[num_tasks][task_name] = np.nanmean(np.stack([
                np.pad(x, (0, max_len - x.shape[0]), mode='constant', constant_values=np.nan)
                for x in d
            ]), axis=0)
            
    from matplotlib import pyplot as plt

    def plot_avg(data_avg, filename):
        fig, axes = plt.subplots(
                ncols=len(data_avg),
                nrows=len(data_avg[1]),
                figsize=(4*len(data_avg), 3*len(data_avg[1])),
                sharey='row', # type: ignore
        )
        # Get list of task names so that we can handle missing data
        all_task_names = set()
        for i, (num_tasks, data) in enumerate(data_avg.items()):
            for j, (task_name, y) in enumerate(sorted(data.items())):
                all_task_names.add(task_name)
        # Plot
        for i, (num_tasks, data) in enumerate(sorted(data_avg.items())):
            for j, task_name in enumerate(sorted(all_task_names)):
                ax = axes[j, i]
                all_task_names.add(task_name)

                if task_name not in data:
                    continue

                y = data[task_name]
                x = np.arange(len(y))

                ax.grid(axis='y', which='both', linestyle=':')
                ax.axhline(0, color='gray', linestyle='--')
                #ax.set_title(f'{task_name} ({num_tasks} tasks)')
                ax.plot(x, y)
        # Labels
        row_labels = sorted(all_task_names)
        col_labels = [f'{x} task(s)' for x in sorted(data_avg.keys())]
        for ax, col in zip(axes[0], col_labels):
            ax.set_title(col)
        for ax, row in zip(axes[:,0], row_labels):
            ax.set_ylabel(row, rotation=90, size='large')
        ## Make sure all plots have the same y-axis scale so we can compare them
        #for j, task_name in enumerate(sorted(all_task_names)):
        #    ylims = []
        #    for i, (num_tasks, data) in enumerate(data_avg.items()):
        #        ax = axes[j, i]
        #        ylims.append(ax.get_ylim())
        #    for i, (num_tasks, data) in enumerate(data_avg.items()):
        #        ax = axes[j, i]
        #        ax.set_ylim([min(y[0] for y in ylims), max(y[1] for y in ylims)])
        #plt.show()
        os.makedirs(output_dir, exist_ok=True)
        plt.savefig(os.path.join(output_dir, filename))
        print(f'Saved plot to {os.path.abspath(os.path.join(output_dir, filename))}')
        plt.close()

    #plot_avg(data_avg_short)
    plot_avg(data_avg_long, 'performance_over_time.png')


def plot_test_vs_train_performance(eval_train_results_dir, eval_train_all_results_dir, eval_results_dir, output_dir, random_performance, single_task_performance):
    """ Produce a plot to see how well the model generalizes as a function of how well it learned the training tasks """
    data_raw = defaultdict(lambda: defaultdict(list)) # {num_tasks: [(train_performance, test_performance)]}
    for filename in os.listdir(eval_results_dir):
        if not filename.endswith('.pt'):
            continue
        num_tasks = os.path.basename(filename).split('.')[0].count('1')
        # Load test results
        test_results = torch.load(os.path.join(eval_results_dir, filename))
        # Load train results
        if num_tasks == 1:
            train_results = torch.load(os.path.join(eval_train_results_dir, filename))
        else:
            train_results = torch.load(os.path.join(eval_train_all_results_dir, filename))
        # Compute point
        def get_normalized_performance(results):
            data = {}
            for task_name, result in results.items():
                data[task_name] = np.mean([r['episode_reward'].item() for r in result])
                min_r = random_performance[task_name]
                max_r = single_task_performance[task_name]
                data[task_name] = (data[task_name] - min_r) / (max_r - min_r)
            return data
        test_performance = get_normalized_performance(test_results)
        train_performance = np.mean(list(get_normalized_performance(train_results).values()))
        for task_name, performance in test_performance.items():
            data_raw[num_tasks][task_name].append((train_performance, performance))

    # Produce scatter plot of train vs test performance
    from matplotlib import pyplot as plt
    
    plt.figure()
    plt.xlabel('Train performance')
    plt.ylabel('Test performance')
    plt.title('Test performance vs train performance')
    plt.grid(axis='both', linestyle=':')
    for num_tasks, data in sorted(data_raw.items()):
        if num_tasks == 1:
            continue
        points = list(itertools.chain(*data.values()))
        x = [p[0] for p in points]
        y = [p[1] for p in points]
        plt.scatter(x, y, label=f'{num_tasks} tasks')
    plt.legend()
    filename = os.path.join(output_dir, f'test_vs_train_performance.png')
    plt.savefig(filename)
    print(f'Saved plot to {os.path.abspath(filename)}')
    plt.close()


def plot_results(output_dir, results_dir):
    eval_results_dir = ResultsDir.from_results_dir(results_dir)
    eval_train_results_dir = os.path.join(results_dir, 'eval_train_results')
    #eval_train_all_results_dir = os.path.join(results_dir, 'eval_train_all_results')
    eval_random_results_dir = os.path.join(results_dir, 'eval_random_results')

    random_performance = get_performance_episode(eval_random_results_dir)
    single_task_performance = get_performance_episode(eval_train_results_dir)

    result_dirs = [
        #('standard setup', eval_results_dir.default),
        #('shuffled observation', eval_results_dir.shuffled_obs),
        #('shuffled action', eval_results_dir.shuffled_action),
        #('shuffled observation and action', eval_results_dir.shuffled_obs_and_action),
        #('occluded obs', eval_results_dir.occluded_obs_100),
        ('occluded obs action and reward', eval_results_dir.occluded_obs_action_reward_100),
    ]
    for description, result_dir in result_dirs:
        print('#'*80)
        print(f'# Test performance with {description}')
        print('#'*80)

        performance_table(eval_results_dir.default, random_performance, single_task_performance)

        if not os.path.exists(eval_results_dir.default):
            print(f'No results found in {eval_results_dir.default}')
            print('  Models have not been evaluated on test tasks yet. Skipping plotting.')
            continue

        plot_performance_episode(
            eval_results_dir=result_dir,
            output_dir=output_dir,
            random_performance=random_performance,
            single_task_performance=single_task_performance,
        )

        #plot_performance_over_time(
        #    eval_results_dir=result_dir,
        #    output_dir=output_dir,
        #    random_performance=random_performance,
        #    single_task_performance=single_task_performance,
        #)

        #plot_test_vs_train_performance(
        #    eval_train_results_dir=eval_train_results_dir,
        #    eval_train_all_results_dir=eval_train_all_results_dir,
        #    eval_results_dir=result_dir,
        #    output_dir=output_dir,
        #    random_performance=random_performance,
        #    single_task_performance=single_task_performance,
        #)


def plot_slurm(argv, after):
    argv = filter(lambda arg: arg != '--slurm', argv)
    slurm_kwargs = {}
    if len(after) > 0:
        slurm_kwargs['dependency'] = dict(afterok=':'.join(str(job_id) for job_id in after))
    slurm = Slurm(
        job_name='plot',
        cpus_per_task=2,
        mem='4G',
        output='/network/scratch/h/huanghow/slurm/%A_%a.out',
        time=datetime.timedelta(days=0, hours=1, minutes=0, seconds=0),
        partition='long-cpu',
        **slurm_kwargs,
    )
    #slurm.add_cmd('module load libffi') # Fixes the "ImportError: libffi.so.6: cannot open shared object file: No such file or directory" error
    slurm.add_cmd('module load python/3.10')
    slurm.add_cmd('source big_rl/ENV/bin/activate')
    slurm.add_cmd('export PYTHONUNBUFFERED=1')
    print('-'*80)
    print(slurm.script())
    print('-'*80)
    job_id = slurm.sbatch('python big_rl_experiments/exp1/__main__.py plot ' + ' '.join(argv))
    return job_id


##################################################
# Main
##################################################


def init_arg_parser():
    parser = argparse.ArgumentParser()

    parser.add_argument('actions', type=str, nargs='*',
                        #choices=['train', 'eval_random', 'eval_train', 'eval_test', 'plot'],
                        default=['train', 'eval_random', 'eval_train', 'eval_train_all', 'eval_test', 'plot'],
                        help='')

    parser.add_argument('--exp-id', type=str, default=None,
                        help='Identifier for the current experiment set.')
    parser.add_argument('--task-counts', type=int, nargs='+', 
                        default=list(range(1, TOTAL_NUM_TASKS)),
                        help='Train on sets of tasks of these sizes.')
    parser.add_argument('--required-tasks', type=int, nargs='+', 
                        default=list(range(TOTAL_NUM_TASKS)),
                        help='Train on sets of tasks that include these tasks. Each set of tasks will be guaranteed to include one of the these tasks.')
    parser.add_argument('--max-steps', type=int, default=50_000_000,
                        help='Maximum number of steps to train for.')
    parser.add_argument('--num-random-evals', type=int, default=100, 
                        help='Number of times to evaluate the random policy.')
    parser.add_argument('--model-config', type=str,
                        default='./big_rl_experiments/exp1/configs/models/model.yaml', 
                        help='Path to the model configs.')
    parser.add_argument('--env-config-dir', type=str,
                        default='./big_rl_experiments/exp1/configs/envs', 
                        help='Directory containing environment configs.')
    parser.add_argument('--checkpoints', type=str, nargs='?', default=None,
                        help='List of checkpoints to evaluate. Used for the eval_train and eval_test actions.')

    parser.add_argument('--slurm', action='store_true',
                        help='Submit job to slurm.')
    parser.add_argument('--debug', action='store_true',
                        help='Run in debug mode.')

    return parser


def main_slurm(args):
    # Select directory based on hostname
    results_dir = os.path.join(os.environ['HOME'], 'results', 'big_rl_experiments', 'exp1', args.exp_id)
    checkpoint_dir = os.path.join(results_dir, 'checkpoints')

    eval_results_dir = ResultsDir.from_results_dir(results_dir)
    #eval_results_dir = os.path.join(results_dir, EVAL_RESULTS_SUBDIR)
    #eval_shuffled_obs_results_dir = os.path.join(results_dir, EVAL_SHUFFLED_OBS_RESULTS_SUBDIR)
    #eval_shuffled_action_results_dir = os.path.join(results_dir, EVAL_SHUFFLED_ACTION_RESULTS_SUBDIR)
    #eval_shuffled_obs_and_action_results_dir = os.path.join(results_dir, EVAL_SHUFFLED_OBS_AND_ACTION_RESULTS_SUBDIR)
    #eval_occluded_obs_100_results_dir = os.path.join(results_dir, EVAL_OCCLUDED_OBS_100_RESULTS_SUBDIR)

    eval_train_results_dir = os.path.join(results_dir, 'eval_train_results')
    eval_train_all_results_dir = os.path.join(results_dir, 'eval_train_all_results')
    eval_random_results_dir = os.path.join(results_dir, 'eval_random_results')
    plots_dir = os.path.join(results_dir, 'plots')

    # Get index of arguments after the list of actions
    argv_start_idx = 0
    for argv_start_idx, argv in enumerate(sys.argv):
        if argv.startswith('--'):
            break

    job_ids = []
    if 'train' in args.actions:
        for num_tasks in args.task_counts:
            print(f'Generating args for {num_tasks} tasks')
            task_args_list = [] # The loop below might lock itself out of valid tasks sets, in which case, we don't want to start any of the experiments. Save args here to make sure they are all properly generated before starting any experiments.
            try:
                for required_tasks in args.required_tasks:
                    # Make sure that each task appears at least once in the training set
                    task_args = generate_training_args(
                        num_tasks = num_tasks,
                        required_tasks = [required_tasks],
                        exp_id = args.exp_id,
                        env_config_dir = args.env_config_dir,
                        model_config = args.model_config,
                        checkpoint_dir = checkpoint_dir,
                        max_steps = args.max_steps,
                        debug = args.debug,
                    )
                    if task_args is None:
                        continue
                    task_args_list.append(task_args)
                for task_args in task_args_list:
                    job_ids.append(run_slurm(task_args))
            except Exception as e:
                print(f'Caught exception: {e}')
                print(f'Failed to generate args for {num_tasks} tasks. Successfully generated args for {len(task_args_list)} tasks.')

    eval_job_ids = []

    # Evaluate randomly initialized models
    if 'eval_random' in args.actions:
        ## Check how many times its been evaluated
        #file_count = sum(
        #    1 if f.endswith('.pt') else 0
        #    for f in os.listdir(eval_random_results_dir)
        #)
        ## XXX: Change this to run everything in series.
        #for _ in range(args.num_random_evals - file_count):
        #    task_args = generate_random_args(
        #        output_dir=eval_random_results_dir,
        #        env_config_dir = args.env_config_dir,
        #        model_config = args.model_config,
        #        debug = args.debug,
        #    )
        #    eval_job_ids.append(eval_slurm(task_args, after=job_ids))
        eval_random_slurm(sys.argv[argv_start_idx:])

    # Evaluate all models that were trained on a single task
    # Test on their training task
    if 'eval_train' in args.actions:
        for filename in os.listdir(checkpoint_dir):
            # Check that only one training task was used for this model
            if filename.count('1') != 1:
                continue
            # If it was trained on a single task, evaluate it on that task
            task_args = generate_eval_training_task_args(
                model_checkpoint = os.path.join(checkpoint_dir, filename),
                output_dir = eval_train_results_dir,
                env_config_dir = args.env_config_dir,
                model_config = args.model_config,
                debug = args.debug,
            )
            if task_args is None:
                continue
            eval_job_ids.append(eval_slurm(task_args, after=job_ids))

    # Evaluate all models that were trained on more than one task
    # Test on their training task
    if 'eval_train_all' in args.actions:
        for filename in os.listdir(checkpoint_dir):
            # Check that only one training task was used for this model
            if filename.count('1') == 1:
                continue
            # If it was trained on a single task, evaluate it on that task
            task_args = generate_eval_training_task_args(
                model_checkpoint = os.path.join(checkpoint_dir, filename),
                output_dir = eval_train_all_results_dir,
                env_config_dir = args.env_config_dir,
                model_config = args.model_config,
                debug = args.debug,
            )
            if task_args is None:
                continue
            eval_job_ids.append(eval_slurm(task_args, after=job_ids))

    # Loop through all checkpoint files and evaluate them
    foo = [
        ('eval_test', eval_results_dir.default, ''),
        ('eval_test_shuffled_obs', eval_results_dir.shuffled_obs, 'shuffled_obs'),
        ('eval_test_shuffled_action', eval_results_dir.shuffled_action, 'shuffled_action'),
        ('eval_test_shuffled_obs_and_action', eval_results_dir.shuffled_obs_and_action, 'shuffled_obs_and_action'),
        ('eval_test_occluded_obs_100', eval_results_dir.occluded_obs_100, 'occluded_obs_100'),
        ('eval_test_occluded_obs_action_reward_100', eval_results_dir.occluded_obs_action_reward_100, 'occluded_obs_action_reward_100'),
    ]
    for action, output_dir, env_config_subdir in foo:
        if action in args.actions:
            for filename in os.listdir(checkpoint_dir):
                if filename.count('0') == 0: # Skip models that were trained on all tasks because the test set would be empty
                    continue
                task_args = generate_eval_args(
                        model_checkpoint=os.path.join(checkpoint_dir, filename),
                        output_dir = output_dir,
                        env_config_dir = os.path.join(args.env_config_dir, 'test/autogenerated', env_config_subdir),
                        model_config = args.model_config,
                        debug = args.debug,
                )
                if task_args is None:
                    continue
                eval_job_ids.append(eval_slurm(task_args))

    # Plot the results
    if 'plot' in args.actions:
        # Call this script (local version) with the same arguments, but with only "plot" as the action
        plot_slurm(sys.argv[argv_start_idx:], after=eval_job_ids)

    print(f'Launched {len(job_ids) + len(eval_job_ids)} jobs (plotting not counted)')
    if len(job_ids) > 0:
        print(f'Training job IDs: {job_ids}')
    if len(eval_job_ids) > 0:
        print(f'Evaluation job IDs: {eval_job_ids}')


def main_local(args):
    if os.uname().nodename == 'howard-pc':
        results_dir = os.path.join(os.environ['HOME'], 'tmp', 'results', 'big_rl_experiments', 'exp1', args.exp_id)
    else:
        results_dir = os.path.join(os.environ['HOME'], 'results', 'big_rl_experiments', 'exp1', args.exp_id)
    checkpoint_dir = os.path.join(results_dir, 'checkpoints')

    eval_results_dir = ResultsDir.from_results_dir(results_dir)
    #eval_results_dir = os.path.join(results_dir, EVAL_RESULTS_SUBDIR)
    #eval_shuffled_obs_results_dir = os.path.join(results_dir, EVAL_SHUFFLED_OBS_RESULTS_SUBDIR)
    #eval_shuffled_action_results_dir = os.path.join(results_dir, EVAL_SHUFFLED_ACTION_RESULTS_SUBDIR)
    #eval_shuffled_obs_and_action_results_dir = os.path.join(results_dir, EVAL_SHUFFLED_OBS_AND_ACTION_RESULTS_SUBDIR)
    #eval_occluded_obs_100_results_dir = os.path.join(results_dir, EVAL_OCCLUDED_OBS_100_RESULTS_SUBDIR)

    eval_train_results_dir = os.path.join(results_dir, 'eval_train_results')
    eval_train_all_results_dir = os.path.join(results_dir, 'eval_train_all_results')
    eval_random_results_dir = os.path.join(results_dir, 'eval_random_results')
    plots_dir = os.path.join(results_dir, 'plots')

    assert isinstance(args.actions, list)

    if 'train' in args.actions:
        for num_tasks in args.task_counts:
            print(f'Generating args for {num_tasks} tasks')
            #for required_tasks in range(TOTAL_NUM_TASKS):
            for required_tasks in args.required_tasks:
                # Make sure that each task appears at least once in the training set
                task_args = generate_training_args(
                    num_tasks = num_tasks,
                    required_tasks = [required_tasks],
                    exp_id = args.exp_id,
                    env_config_dir = args.env_config_dir,
                    model_config = args.model_config,
                    checkpoint_dir = checkpoint_dir,
                    max_steps = args.max_steps,
                    debug = args.debug,
                )
                if task_args is None:
                    continue

                #run_local(task_args)
                run_subprocess(task_args)
                #run_slurm(task_args)

    # Evaluate randomly initialized models
    if 'eval_random' in args.actions:
        for _ in range(args.num_random_evals):
            task_args = generate_random_args(
                output_dir=eval_random_results_dir,
                env_config_dir = args.env_config_dir,
                model_config = args.model_config,
                debug = args.debug,
            )
            eval_local(task_args)

    # Get all checkpoint files
    if args.checkpoints is not None or not os.path.exists(checkpoint_dir):
        checkpoint_filenames = args.checkpoints
    else:
        checkpoint_filenames = os.listdir(checkpoint_dir)

    # Evaluate all models that were trained on a single task
    # Test on their training task
    if 'eval_train' in args.actions:
        for filename in checkpoint_filenames:
            if filename.count('1') != 1:
                continue
            task_args = generate_eval_training_task_args(
                model_checkpoint = os.path.join(checkpoint_dir, filename),
                output_dir = eval_train_results_dir,
                env_config_dir = args.env_config_dir,
                model_config = args.model_config,
                debug = args.debug,
            )
            if task_args is None:
                continue
            eval_local(task_args)
            
    if 'eval_train_all' in args.actions:
        for filename in checkpoint_filenames:
            if filename.count('1') == 1: # Skip these because they are already evaluated above
                continue
            task_args = generate_eval_training_task_args(
                model_checkpoint = os.path.join(checkpoint_dir, filename),
                output_dir = eval_train_all_results_dir,
                env_config_dir = args.env_config_dir,
                model_config = args.model_config,
                debug = args.debug,
            )
            if task_args is None:
                continue
            eval_local(task_args)

    # Loop through all checkpoint files and evaluate them
    foo = [
        ('eval_test', eval_results_dir.default, ''),
        ('eval_test_shuffled_obs', eval_results_dir.shuffled_obs, 'shuffled_obs'),
        ('eval_test_shuffled_action', eval_results_dir.shuffled_action, 'shuffled_action'),
        ('eval_test_shuffled_obs_and_action', eval_results_dir.shuffled_obs_and_action, 'shuffled_obs_and_action'),
        ('eval_test_occluded_obs_100', eval_results_dir.occluded_obs_100, 'occluded_obs_100'),
        ('eval_test_occluded_obs_action_reward_100', eval_results_dir.occluded_obs_action_reward_100, 'occluded_obs_action_reward_100'),
    ]
    for action, output_dir, env_config_subdir in foo:
        if action in args.actions:
            for filename in checkpoint_filenames:
                if filename.count('0') == 0: # Skip models that were trained on all tasks because the test set would be empty
                    continue
                task_args = generate_eval_args(
                        model_checkpoint=os.path.join(checkpoint_dir, filename),
                        output_dir = output_dir,
                        env_config_dir = os.path.join(args.env_config_dir, env_config_subdir),
                        model_config = args.model_config,
                        debug = args.debug,
                )
                if task_args is None:
                    continue
                eval_local(task_args)

    # Plot the results
    if 'plot' in args.actions:
        plot_results(
            output_dir=plots_dir,
            results_dir=results_dir,
        )


def main():
    parser = init_arg_parser()
    args = parser.parse_args()

    if args.slurm:
        main_slurm(args)
    else:
        main_local(args)


if __name__ == '__main__':
    main()
