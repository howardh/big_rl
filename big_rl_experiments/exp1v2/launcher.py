import argparse
import datetime
import os
import logging
import subprocess
import uuid

import numpy as np
from simple_slurm import Slurm

from big_rl_experiments.exp1v2.common import ResultsDir, TOTAL_NUM_TASKS


logger = logging.getLogger(__name__)

SHELL = '/bin/bash' # bash is needed because `trap` behaves differently in sh and the `module` command isn't available in sh.
GRES = 'gpu:a100l.2g.20gb:1'
TRAINING_STEPS = 50_000_000
TRANSFER_STEPS = 1_000_000

UBUNTU2204 = True


##################################################
# Generate args
##################################################


def generate_training_args(exp_id, env_config_dir, model_config, checkpoint_dir, max_steps: int, num_tasks: int, required_tasks: list[int], num_configs: int, debug: bool = False) -> list[list[str]]:
    # Get all task config files
    filenames = os.listdir(os.path.join(env_config_dir, 'train/autogenerated'))
    filenames = [f for f in filenames if f.endswith('.yaml')]
    np.random.shuffle(filenames)

    selected_configs = []
    output_args = []
    excluded_tasks = np.array([False] * TOTAL_NUM_TASKS) # Flag to mark that at least one selected config does not include a given task
    included_tasks = np.array([True] * TOTAL_NUM_TASKS) # Flag to mark that at least one selected config includes a given task. If it's not a required task, then we set it to True so we won't be trying to find configs with that task.
    for t in required_tasks:
        included_tasks[t] = False

    for filename in filenames:
        if len(selected_configs) >= num_configs:
            break

        # Count number of tasks (1s) in the filename
        tasks_in_config = np.array([c == '1' for c in filename.split('.')[0]])
        num_tasks_in_config = sum(tasks_in_config)
        if num_tasks_in_config != num_tasks:
            logger.debug(f'{filename} rejected. Number of tasks ({num_tasks_in_config}) does not match the number of tasks requested ({num_tasks}).')
            continue
        # Check if at least one of the required tasks are in the config and haven't been included yet in the selected configs
        # But only if there are tasks left to include. Otherwise, we can just include all configs.
        included_tasks_match = np.logical_and(
                ~included_tasks, tasks_in_config)
        excluded_tasks_match = np.logical_and(
                ~excluded_tasks, ~tasks_in_config)
        if ((~included_tasks).sum() > 0 and included_tasks_match.sum() == 0) and ((~excluded_tasks).sum() > 0 and excluded_tasks_match.sum() == 0):
            logger.debug(f'{filename} rejected. Does not include any of the required tasks.')
            continue

        # Check if the model checkpoint exists
        # If it does, it means we've already trained on this set of tasks, so we should look for another one
        model_checkpoint = os.path.join(checkpoint_dir, filename.replace('.yaml', '.pt'))
        if os.path.exists(model_checkpoint):
            continue

        # Found a config that matches the criteria
        selected_configs.append(filename)
        print('', filename)
        excluded_tasks = np.logical_or(excluded_tasks, excluded_tasks_match)
        included_tasks = np.logical_or(included_tasks, included_tasks_match)

        env_config = os.path.join(
                env_config_dir, 'train/autogenerated', filename)
        run_id = f'{exp_id}-{filename.replace(".yaml", "")}'
        args = [
            '--env-config', env_config,
            '--model-config', model_config,
            '--model-checkpoint', model_checkpoint,
            '--checkpoint-interval', '1_000_000',
            '--run-id', run_id,
            '--wandb-id', run_id,
            '--max-steps-total',
                ('30000' if debug else str(max_steps)),
            '--cuda',
        ]
        if not debug:
            args.append('--wandb')
        output_args.append(args)

    if len(selected_configs) < num_configs:
        raise RuntimeError(f'Could not find {num_configs} configs with {num_tasks} tasks')

    return output_args


def generate_random_args(output_dir, env_config_dir, model_config, debug) -> list[str]:
    # Test randomly initialized models.
    # Get a score for normalization purposes.
    env_config_dir = os.path.join(env_config_dir, 'test/autogenerated')
    # Choose first file and replace 0s with 1s to get the set with all tasks
    env_config = os.path.join(env_config_dir, os.listdir(env_config_dir)[0].replace('0', '1'))
    # Random results file name
    results = os.path.join(output_dir, f'{uuid.uuid4()}.pt')

    args = [
        '--env-config', env_config,
        '--model-config', model_config,
        '--results', results,
        '--num-episodes', '1',
        '--no-video',
    ]
    return args


def generate_eval_training_task_args(model_checkpoint: str, output_dir: str, env_config_dir, model_config, debug) -> list[str] | None:
    # Test models on the training tasks.
    # Get a score for normalization purposes.
    env_config = os.path.join(env_config_dir, 'test/autogenerated', os.path.basename(model_checkpoint).replace('.pt', '.yaml'))

    results = os.path.join(output_dir, os.path.basename(model_checkpoint))
    if os.path.exists(results):
        print(f'  {results} already exists')
        return None

    args = [
        '--env-config', env_config,
        '--model-config', model_config,
        '--model', model_checkpoint,
        '--results', results,
        '--num-episodes', '10',
        '--no-video',
    ]
    return args


def generate_eval_args(model_checkpoint: str, output_dir, env_config_dir, model_config, test_set: bool, debug) -> list[str] | None:
    if test_set:
        # Evaluate on the set of tasks complementary to the training set
        env_config = os.path.join(
            env_config_dir,
            ''.join([
                '1' if c == '0' else '0' if c == '1' else c
                for c in os.path.basename(model_checkpoint)
            ]).replace('.pt', '.yaml')
        )
    else:
        # Evaluate on the same tasks it was trained on
        env_config = os.path.join(
            env_config_dir,
            os.path.basename(model_checkpoint).replace('.pt', '.yaml')
        )
    print(f'{os.path.basename(model_checkpoint)} -> {env_config}')

    results = os.path.join(output_dir, os.path.basename(model_checkpoint))
    if os.path.exists(results):
        print(f'  {results} already exists')
        return None

    args = [
        '--env-config', env_config,
        '--model-config', model_config,
        '--model', model_checkpoint,
        '--results', results,
        '--num-episodes', '10',
        '--no-video',
    ]
    return args


def generate_transfer_args(exp_id, env_config_dir, model_config, checkpoint_dir, checkpoint_transfer_dir: str, debug: bool = False) -> list[list[str]]:
    # Train for 1mil steps, then save the model
    # Loop through existing checkpoints
    # For each checkpoint, transfer to each of the single tasks

    # How can I ensure that the model was done training on the original 50mil steps before I start this?
    # The original 50mil steps isn't exactly 50mil. How do I make sure this trains for exactly 1mil rather than than 1mil plus however short from 50mil it was?
    # Do I load the model and check the number of steps it was trained for? Then add exactly 1mil to that?
    # I think the solution is to just train for 1mil steps and then save the model at the end with no intermediary checkpoints. Then I can be sure that the model was trained for exactly 1mil steps. 1mil steps is short enough that it's not a big deal to restart if needed.
    #raise NotImplementedError()

    # Loop through existing checkpoints
    output = []
    for filename in os.listdir(checkpoint_dir):
        starting_model = os.path.join(checkpoint_dir, filename)
        training_tasks = [i for i, c in enumerate(filename.split('.')[0]) if c == '1']

        # Loop through all possible tasks it could be transferred to
        def make_filename(task):
            output = ['0'] * TOTAL_NUM_TASKS
            output[task] = '1'
            return ''.join(output) + '.yaml'
        target_task_filenames = [
            make_filename(task)
            for task in range(TOTAL_NUM_TASKS)
            if task not in training_tasks
        ]

        for target_task_filename in target_task_filenames:
            model_checkpoint = os.path.join(
                checkpoint_transfer_dir, f'{filename.replace(".pt", "")}_{target_task_filename.replace(".yaml", "")}.pt')
            if os.path.exists(model_checkpoint):
                print(f'{model_checkpoint} already exists')
                continue
            env_config = os.path.join(
                    env_config_dir, 'train/autogenerated', filename)
            run_id = f'{exp_id}-{filename.replace(".yaml", "")}_{target_task_filename.replace(".yaml", "")}'
            args = [
                '--env-config', env_config,
                '--model-config', model_config,
                '--starting-model', starting_model,
                '--model-checkpoint', model_checkpoint,
                '--checkpoint-interval', str(TRANSFER_STEPS * 2), # Checkpoint only gets saved at the end of training
                '--run-id', run_id,
                '--wandb-id', run_id,
                '--max-steps',
                    ('1000' if debug else str(TRANSFER_STEPS)),
                '--cuda',
            ]
            if not debug:
                args.append('--wandb')
            output.append(args)

            print(f'{filename} -> {target_task_filename}')
    print('total tasks', len(output))
    return output


##################################################
# Main
##################################################


def run_train(args: list[str], slurm: bool = False):
    if slurm:
        slurm_kwargs = {}
        if UBUNTU2204:
            slurm_kwargs['reservation'] = 'ubuntu2204'
        s = Slurm(
            job_name='train',
            cpus_per_task=8,
            mem='8G',
            gres=[GRES],
            output='/network/scratch/h/huanghow/slurm/%A_%a.out',

            # Run for five days
            array='1-5%1',
            time=datetime.timedelta(days=1, hours=0, minutes=0, seconds=0),

            #partition='main',
            signal='USR1@120', # Send a signal to the job 120 seconds before it is killed

            **slurm_kwargs,
        )
        #slurm.add_cmd('module load libffi') # Fixes the "ImportError: libffi.so.6: cannot open shared object file: No such file or directory" error
        s.add_cmd('module load python/3.10')
        if UBUNTU2204:
            s.add_cmd('source big_rl/ENV2204/bin/activate')
        else:
            s.add_cmd('source big_rl/ENV/bin/activate')
        s.add_cmd('export PYTHONUNBUFFERED=1')
        #ps://stackoverflow.com/questions/76348342/how-to-use-trap-in-my-sbatch-bash-job-script-in-compute-canada
        s.add_cmd("trap 'echo SIGUSR1 1>&2' SIGUSR1") # Handles time limit
        s.add_cmd("trap 'echo SIGUSR1 1>&2' SIGTERM") # Handles preemption (I think this is needed if PreemptParameters isn't set with send_user_signal enabled. Check if it's set in /etc/slurm/slurm.conf)
        cmd = 'srun python big_rl/generic/script.py ' + ' '.join(args)

        print('-'*80)
        print(s.script(shell=SHELL))
        print(cmd)
        print('-'*80)

        job_id = s.sbatch(cmd, shell=SHELL)
        return job_id
    else:
        script = 'big_rl_experiments/exp1v2/__main__.py'
        action = 'train'
        cmd = f'python3 {script} {action} {" ".join(args)}'

        print(cmd)

        p = subprocess.Popen(cmd, shell=True) # if shell=True, then the subprocesses will have the same environment variables as this process. Needed to pass the CUDA_VISIBLE_DEVICES variable.
        p.wait()


def run_transfer(args: list[str], slurm: bool = False):
    if slurm:
        slurm_kwargs = {}
        if UBUNTU2204:
            slurm_kwargs['reservation'] = 'ubuntu2204'
        s = Slurm(
            job_name='train',
            cpus_per_task=8,
            mem='8G',
            gres=[GRES],
            output='/network/scratch/h/huanghow/slurm/%A_%a.out',

            # Run for five days
            #array='1-5%1',
            time=datetime.timedelta(days=1, hours=0, minutes=0, seconds=0),

            #partition='main',
            signal='USR1@120', # Send a signal to the job 120 seconds before it is killed

            **slurm_kwargs,
        )
        #slurm.add_cmd('module load libffi') # Fixes the "ImportError: libffi.so.6: cannot open shared object file: No such file or directory" error
        s.add_cmd('module load python/3.10')
        if UBUNTU2204:
            s.add_cmd('source big_rl/ENV2204/bin/activate')
        else:
            s.add_cmd('source big_rl/ENV/bin/activate')
        s.add_cmd('export PYTHONUNBUFFERED=1')
        #ps://stackoverflow.com/questions/76348342/how-to-use-trap-in-my-sbatch-bash-job-script-in-compute-canada
        s.add_cmd("trap 'echo SIGUSR1 1>&2' SIGUSR1") # Handles time limit
        s.add_cmd("trap 'echo SIGUSR1 1>&2' SIGTERM") # Handles preemption (I think this is needed if PreemptParameters isn't set with send_user_signal enabled. Check if it's set in /etc/slurm/slurm.conf)
        cmd = 'srun python big_rl/generic/script.py ' + ' '.join(args)

        print('-'*80)
        print(s.script(shell=SHELL))
        print(cmd)
        print('-'*80)

        job_id = s.sbatch(cmd, shell=SHELL)
        return job_id
    else:
        script = 'big_rl_experiments/exp1v2/__main__.py'
        action = 'train'
        cmd = f'python3 {script} {action} {" ".join(args)}'

        print(cmd)

        p = subprocess.Popen(cmd, shell=True) # if shell=True, then the subprocesses will have the same environment variables as this process. Needed to pass the CUDA_VISIBLE_DEVICES variable.
        p.wait()


def run_eval(args: list[str], slurm: bool = False, after=[]):
    if slurm:
        slurm_kwargs = {}
        if len(after) > 0:
            slurm_kwargs['dependency'] = dict(afterok=':'.join(str(job_id) for job_id in after))
        if UBUNTU2204:
            slurm_kwargs['reservation'] = 'ubuntu2204'
        s = Slurm(
            job_name='eval',
            cpus_per_task=8,
            mem='8G',
            output='/network/scratch/h/huanghow/slurm/%A_%a.out',
            time=datetime.timedelta(days=0, hours=2, minutes=0, seconds=0),
            partition='long-cpu',
            **slurm_kwargs,
        )
        #slurm.add_cmd('module load libffi') # Fixes the "ImportError: libffi.so.6: cannot open shared object file: No such file or directory" error
        s.add_cmd('module load python/3.10')
        if UBUNTU2204:
            s.add_cmd('source big_rl/ENV2204/bin/activate')
        else:
            s.add_cmd('source big_rl/ENV/bin/activate')
        s.add_cmd('export PYTHONUNBUFFERED=1')
        cmd = 'python big_rl/generic/evaluate_model.py ' + ' '.join(args)
        print('-'*80)
        print(s.script(shell=SHELL))
        print(cmd)
        print('-'*80)
        job_id = s.sbatch(cmd, shell=SHELL)
        return job_id
    else:
        script = 'big_rl_experiments/exp1v2/__main__.py'
        action = 'eval'
        cmd = f'python3 {script} {action} {" ".join(args)}'

        print(cmd)

        p = subprocess.Popen(cmd, shell=True) # if shell=True, then the subprocesses will have the same environment variables as this process. Needed to pass the CUDA_VISIBLE_DEVICES variable.
        p.wait()


def run_plot(args: list[str], slurm: bool = False):
    if slurm:
        ...
    else:
        script = 'big_rl_experiments/exp1v2/__main__.py'
        action = 'plot'
        cmd = f'python3 {script} {action} {" ".join(args)}'

        print(cmd)

        p = subprocess.Popen(cmd, shell=True) # if shell=True, then the subprocesses will have the same environment variables as this process. Needed to pass the CUDA_VISIBLE_DEVICES variable.
        p.wait()


def launch(args):
    if os.uname().nodename == 'howard-pc':
        results_dir = os.path.join(os.environ['HOME'], 'tmp', 'results', 'big_rl_experiments', 'exp1', args.exp_id)
    else:
        results_dir = os.path.join(os.environ['HOME'], 'results', 'big_rl_experiments', 'exp1', args.exp_id)

    checkpoint_dir = os.path.join(results_dir, 'checkpoints')
    transfer_checkpoint_dir = os.path.join(results_dir, 'checkpoints_transfer')

    eval_test_results_dir = ResultsDir.from_results_dir(os.path.join(results_dir, 'eval_test_results'))
    eval_train_results_dir = ResultsDir.from_results_dir(os.path.join(results_dir, 'eval_train_results'))

    #eval_train_results_dir = os.path.join(results_dir, 'eval_train_results')
    eval_random_results_dir = os.path.join(results_dir, 'eval_random_results')
    plots_dir = os.path.join(results_dir, 'plots')

    assert isinstance(args.actions, list)

    train_job_ids = []
    eval_job_ids = []
    plot_job_ids = []

    if 'train' in args.actions:
        for num_tasks in args.task_counts:
            print(f'Generating args for {num_tasks} tasks')
            task_args_list = generate_training_args(
                num_tasks = num_tasks,
                num_configs = TOTAL_NUM_TASKS,
                required_tasks = args.required_tasks,
                exp_id = args.exp_id,
                env_config_dir = args.env_config_dir,
                model_config = args.model_config,
                checkpoint_dir = checkpoint_dir,
                max_steps = args.max_steps,
                debug = args.debug,
            )
            for task_args in task_args_list:
                job_id = run_train(task_args, slurm=args.slurm)
                train_job_ids.append(job_id)

    # Evaluate randomly initialized models
    if 'eval_random' in args.actions:
        for _ in range(args.num_random_evals):
            task_args = generate_random_args(
                output_dir=eval_random_results_dir,
                env_config_dir = args.env_config_dir,
                model_config = args.model_config,
                debug = args.debug,
            )
            job_id = run_eval(task_args, slurm=args.slurm)
            eval_job_ids.append(job_id)

    # Get all checkpoint files
    if args.checkpoints is not None or not os.path.exists(checkpoint_dir):
        checkpoint_filenames = args.checkpoints
    else:
        checkpoint_filenames = os.listdir(checkpoint_dir)

    ## Evaluate all models that were trained on a single task
    ## Test on their training task
    #if 'eval_train' in args.actions:
    #    for filename in checkpoint_filenames:
    #        if filename.count('1') != 1:
    #            continue
    #        task_args = generate_eval_training_task_args(
    #            model_checkpoint = os.path.join(checkpoint_dir, filename),
    #            output_dir = eval_train_results_dir.default,
    #            env_config_dir = args.env_config_dir,
    #            model_config = args.model_config,
    #            debug = args.debug,
    #        )
    #        if task_args is None:
    #            continue
    #        job_id = run_eval(task_args, slurm=args.slurm)
    #        eval_job_ids.append(job_id)

    # Loop through all checkpoint files and evaluate them on the test set
    foo = [
        ('eval_test', eval_test_results_dir.default, ''),
        ('eval_test_shuffled_obs', eval_test_results_dir.shuffled_obs, 'shuffled_obs'),
        ('eval_test_shuffled_action', eval_test_results_dir.shuffled_action, 'shuffled_action'),
        ('eval_test_shuffled_obs_and_action', eval_test_results_dir.shuffled_obs_and_action, 'shuffled_obs_and_action'),
        ('eval_test_occluded_obs_100', eval_test_results_dir.occluded_obs_100, 'occluded_obs_100'),
        ('eval_test_occluded_obs_action_reward_100', eval_test_results_dir.occluded_obs_action_reward_100, 'occluded_obs_action_reward_100'),
    ]
    for action, output_dir, env_config_subdir in foo:
        if action in args.actions:
            for filename in checkpoint_filenames:
                if filename.count('0') == 0: # Skip models that were trained on all tasks because the test set would be empty
                    continue
                task_args = generate_eval_args(
                        model_checkpoint=os.path.join(checkpoint_dir, filename),
                        output_dir = output_dir,
                        env_config_dir = os.path.join(args.env_config_dir, 'test/autogenerated', env_config_subdir),
                        model_config = args.model_config,
                        test_set = True,
                        debug = args.debug,
                )
                if task_args is None:
                    continue
                job_id = run_eval(task_args, slurm=args.slurm)
                eval_job_ids.append(job_id)

    # Loop through all checkpoint files and evaluate them on the training set
    foo = [
        ('eval_train', eval_train_results_dir.default, ''),
        ('eval_train_shuffled_obs', eval_train_results_dir.shuffled_obs, 'shuffled_obs'),
        ('eval_train_shuffled_action', eval_train_results_dir.shuffled_action, 'shuffled_action'),
        ('eval_train_shuffled_obs_and_action', eval_train_results_dir.shuffled_obs_and_action, 'shuffled_obs_and_action'),
        ('eval_train_occluded_obs_100', eval_train_results_dir.occluded_obs_100, 'occluded_obs_100'),
        ('eval_train_occluded_obs_action_reward_100', eval_train_results_dir.occluded_obs_action_reward_100, 'occluded_obs_action_reward_100'),
    ]
    for action, output_dir, env_config_subdir in foo:
        if action in args.actions:
            for filename in checkpoint_filenames:
                task_args = generate_eval_args(
                        model_checkpoint=os.path.join(checkpoint_dir, filename),
                        output_dir = output_dir,
                        env_config_dir = os.path.join(args.env_config_dir, 'test/autogenerated', env_config_subdir),
                        model_config = args.model_config,
                        test_set = False,
                        debug = args.debug,
                )
                if task_args is None:
                    continue
                job_id = run_eval(task_args, slurm=args.slurm)
                eval_job_ids.append(job_id)

    # Transfer to all single tasks
    if 'transfer_test' in args.actions:
        task_args_list = generate_transfer_args(
            exp_id = args.exp_id,
            env_config_dir = args.env_config_dir,
            model_config = args.model_config,
            checkpoint_dir = checkpoint_dir,
            checkpoint_transfer_dir = transfer_checkpoint_dir,
            debug = args.debug,
        )
        for task_args in task_args_list:
            run_transfer(task_args, slurm=args.slurm)
            break # XXX: DEBUG
            #continue

    # Plot the results
    if 'plot' in args.actions:
        #plot_results(
        #    output_dir=plots_dir,
        #    results_dir=results_dir,
        #)
        run_plot([
            '--results-dir', results_dir,
            '--output-dir', plots_dir,
        ])
        #raise NotImplementedError()

    print(f'Launched {len(train_job_ids) + len(eval_job_ids)} jobs (plotting not counted)')
    if len(train_job_ids) > 0:
        print(f'Training job IDs: {train_job_ids}')
    if len(eval_job_ids) > 0:
        print(f'Evaluation job IDs: {eval_job_ids}')


def init_arg_parser():
    parser = argparse.ArgumentParser()

    parser.add_argument('actions', type=str, nargs='*',
                        #choices=['train', 'eval_random', 'eval_train', 'eval_test', 'plot'],
                        default=['train', 'eval_random', 'eval_train', 'eval_train_all', 'eval_test', 'plot'],
                        help='')

    parser.add_argument('--exp-id', type=str, required=True,
                        help='Identifier for the current experiment set.')
    parser.add_argument('--task-counts', type=int, nargs='+', 
                        default=list(range(1, TOTAL_NUM_TASKS)),
                        help='Train on sets of tasks of these sizes.')
    parser.add_argument('--required-tasks', type=int, nargs='+', 
                        default=list(range(TOTAL_NUM_TASKS)),
                        help='Train on sets of tasks that include these tasks. Each set of tasks will be guaranteed to include one of the these tasks.')
    parser.add_argument('--max-steps', type=int, default=50_000_000,
                        help='Maximum number of steps to train for.')
    parser.add_argument('--num-random-evals', type=int, default=100, 
                        help='Number of times to evaluate the random policy.')
    parser.add_argument('--model-config', type=str,
                        default='./big_rl_experiments/exp1/configs/models/model.yaml', 
                        help='Path to the model configs.')
    parser.add_argument('--env-config-dir', type=str,
                        default='./big_rl_experiments/exp1/configs/envs', 
                        help='Directory containing environment configs.')
    parser.add_argument('--checkpoints', type=str, nargs='?', default=None,
                        help='List of checkpoints to evaluate. Used for the eval_train and eval_test actions.')

    parser.add_argument('--slurm', action='store_true',
                        help='Submit job to slurm.')
    parser.add_argument('--debug', action='store_true',
                        help='Run in debug mode.')

    return parser


if __name__ == '__main__':
    parser = init_arg_parser()
    args = parser.parse_args()

    launch(args)
